context-free syntax is specified with a context-free grammar

formally CFG G=(Vt, Vn, S, P), where
- Vt set of terminal symbols (tokens returned by the scanner)
- Vn set of non-terminals symbols (variables that denote sets of substrings occurring in the language)
- S belonging to Vn, the goal non-terminal symbol ~ denote the entire set of strings in L(G)
- P set of productions (specyfying how terminals and non-terminals can be combined to form strings in the language.
..each production must have a single non-terminal on its left hand side
V = Vt U Vn

Syntax analysis
- grammars are often written in backus naur form (BNF)
	
				<goal> ::= <expr>
				<expr> ::= <expr><op><expr> | num | id
				<op> ::= + | - | * | /

- in a BNF for a grammar we represent 
	1. non-terminals with <anglebrackets> or CAPITAL LETTERS
	2. terminals with current font
	3. productions as in the example

Scanning vs. parsing
- regular expressions:
	-used to classify id, numbers, keywords...
	-simpler and more concisse for tokens than a grammar
	-more efficient scanners can be built from REs

	term ::= [a-zA-Z]([a-zA-Z]|[0-9])* | 0 | [1-9][0-9]*
	op ::= + | - | * | /
	expr ::= (term op)* term

-CFGs are used to impose structure
	-brackets: (), begin .. end, if .. then .. else
	-expressions, declarations

			[Factoring out lexical analysis simplifies the compiler]

Hierarchy of grammar classes
- LL(k), LR(k), SLR, LALR

Derivations
- we can view the productions of a CFG as rewriting rules
- <goal> =>* id + num * id (derivation or parse)
- the process of discovering a derivation is called parsing

Derivation
- at each step, we choose a non-terminal to replace
		-> can lead to different derivations
- two interesting strategies
		-> leftmost derivation
		-> rightmost derivation

Precedence
- conceptually based on a treewalk evaluation
- problem: our grammar has no notion of precedence, or implied order of evaluation
		->to add precedence takes additional machinery

				<goal> ::= 	<expr>
				<expr> ::= 	<expr> + <term> 
						|  	<expr> - <term>
						|	<term>
				<term> ::=	<term> * <factor>
						|	<term> / <factor>
						|	<factor>
				<factor>::=	num | id

- this grammar enforces a precedence on the derivation:
		-> terms must be derived from expressions
		-> forces the "correct" tree

Ambiguity
- if a grammar has more than one derivation for a single sentential form, then it is ambiguous
[NB: Given a grammar G=<Vt,Vn,S,P> if S =>* B where B belongs to V*, then B is said to be a sentential form of G]

				<stmt> ::= 	if <expr> then <stmt>
						|	if <expr then <stmt> else <stmt>
						|	...

	-> consider: if E1 then if E2 then S1 else S2
	-> two derivations; the ambiguity is purely grammatical
	-> CONTEXT-FREE AMBIGUITY
	->	(1) <stmt> -> if <expr> then <stmt> -> 	if <expr> then if <expr> then <stmt> else <stmt>
		(2) <stmt> -> if <expr> then <stmt> else <stmt> -> if <expr> then if <expr> then <stmt> else <stmt>

Resolving Ambiguity
- ambiguity may be eliminated by rearranging the grammar
				<stmt>		:= 	<matched> | <unmatched>
				<matched>	:=	if <expr> then <matched> else <matched>
							 |	...
				<unmatched>	:=	if <expr> then <stmt>
							 |	if <expr> then <matched> else <unmatched>
- this generates the same language as the ambiguous grammar, but applies the common sense rule:
		-> match each else with the closest unmatched then

Ambiguity
- ambiguity is often due to confusion in the context-free specification. Confusion can arise from overloading, e.g:
		a = f(17)
- in many languages, f could be a function or a subscripted variable.
- disambiguating this statement requires CONTEXT:
		-> need values of declarations
		-> not context-free
		-> really an issue of type
[rather than complicate parsing, we will handle this SEPARATELY]

Parsing: the big picture
- our goal is a flexible parser generator system:
		grammar-->
		code----->	parser generator--> 							
					tokens ----------->  parser ---> IR

Top-down vs Bottom-up
-Top-down parser:
	- starts at the root of derivation tree and fills in
	- picks a production and tries to match the input
	- may require backtracking
	- some grammars are backtrack-free (predictive)
-Bottom-up parser:
	- starts at the leaves and fills in
	- starts in a state valid for legal first tokens
	- as input is consumed, changes state to encode possibilities (recognize valid prefixes)
	- uses a stack to store both state and sentential forms

Top-down parsing
- a top-down parser starts with the root of the parse tree, labeled with the start or goal symbol of the grammar.
- to build a parse, it repeats the following steps until the fringe of the parse tree matches the input string
	1. at a node labeled A, select a production A->a and construct the appropriate child for each sybol of a
	2. when a terminal is added to the fringe that doesn't match the input string, backtrack
	3. find the next node to be expanded (must have a label in Vn)
- the key is selecting the right production in step 1.
		-> should be guided by input string

Non-termination
- if the parser makes the wrong choices, expansion doesn't terminate!

Left-recursion
		[top-down parsers cannot handle left-recursion in a grammar]
- formally a grammar is LEFT-RECURSIVE if:
		for each A belonging to Vn s.t. A=>+ Aa for some string a belonging to V

Eliminating left-recursion
- we have to remove left-recursion. Need to trasform the grammar
		<foo> ::= <foo>a | b becomes <foo> ::= b<bar> and <bar> ::= a<bar> | ""

Look-ahead
- How much look-ahead is needed?
	- we know that top-down parsers may need to backtrack when they select the wrong production
	- do we need arbitrary look-ahead to parse CFGs?
		-> generally yes
		-> use the Earley or Cocke-Younger, Kasami algorithms ~~ dragon book chapter 4, probl 2.3.4 
	- fortunately
		-> large subclasses of CFGs can be parsed with limited lookahead
		-> most programming language constructs can be expressed in a grammar that falls in these subclasses
	- among the interesting subclasses are:
		->LL(1): left to right scan, left-most derivation, 1-token look-ahead
		->LR(1): left to right scan, right-most derivation, 1-token look-ahead

Predictive parsing
- basic idea:
	-> for any two production A -> a | b where b€V* and a€V*, we would like a distinct way of choosing the correct production to expand.
- for some RHS a€G (where a€V*), define FIRST(a) as the set of tokens (thus non-terminals) that appear first in some string derived from a.
	-> i.e., for some w€Vt*, w€FIRST(a) iff a=>*wy where y€V*
-key property:
	-> whenever two productions A->a|b (where b€V* and a€V*) both appear in the grammar, we would like:
		FIRST(a) intersecated FIRST(b) = empty
	this would allow the parser to make a correct choice with a look-ahead of only one symbol!

Left factoring
- what if a grammar does not have this property?
-> sometimes, we can transform a grammar to have this property:
	- for each non-terminal A find the longest prefix a (a€V*) common to two or more of its alternatives
	- if a != epsilon then replace all of the A productions
		A -> ab1 | ab2 | ... | abn
	with
		A -> aA'
		A' -> b1 | b2 | ... | bn
	- repeat until no two alternatives for a single non-terminal have a common prefix

Back to left-recursion elimination
- given a left-factored CFG, to eliminate left recursion:
	-> if exists A -> Aa (where a€V*) then replace all of the A productions 
		A-> Aa | b | ... | y (where a,b,...,y€V*)
	with
		A-> NA'
		N-> b | ... | y
		A'-> aA' | epsilon
	-> repeat until there are no left-recursive productions

Generality
- Question: By left factoring and eliminating left-recursion, can we transform an arbitrary context-free grammar to a form where it can be predectively parsed with a single token look-ahead?
- Answer: Given a context-free grammar that doesn't meet our conditions, it is undecidable wheter an equivalent grammar exists that does meet our conditions

--> many context-free languages do not have such a grammar:
		{a^n 0 b^n | n>1} U { a^n 1 b^2n | n>= 1}
	- must look past an arbitrary number of a's to discover the 0 or the 1 and so determine the derivation
 
-------------------------------------------------------

				<goal> ::= 	<expr>
				<expr> ::=	<term><expr'>
				<expr'> ::= +<expr> 
						|  	-<expr>
						|	epsilon
				<term>	::=	<factor><term'>
				<term'> ::=	*<term>
						|	/<term>
						|	epsilon
				<factor>::=	num | id


----> Table-driven parsing <----
Recursive descent parsing
- now we can produce a simple recursive descent parser from the above (right-associative) grammar.

	goal:	token <- next_token();
			if (expr() == error | toke
			n != EOF)
				return error
	expr:	if (term() == error) then
				return error;
			else return expr_prime()
	expr_prime:
			if (token = plus) then
				token <- next_token()
				return expr();
			else if (token = minus) then
				token <- next_token();
				return expr()
			else return ok
	term:	if (factor() = error)
				return error
			else return term_prime
	term_prime:
			if (token = mult)
				token<-next_token();
				return term()
			else if (token = div) then
				token <- next_token()
				return term()
			else return ok;
	factor:
			if (token = num) then
				token<-next_token()
				return ok
			else if (token = id) then
				token <- next_token()
				return ok
			else return error

Building the tree
-> one of the key jobs of the parser is to build an intermediate representation (IR) of the source code

-> to build an abstract syntax tree, we can simply insert code at the appropriate points:
	- factor() can stack nodes id, num
	- term_prime() can stack nodes *, /
	- term() can pop 3, build and push subtree
	- expr_prime() can stack nodes +,-
	- expr() can pop 3, build and push subtree
	- goal() can pop and return tree

Non-recursive predictive parsing
-> observation: our recursive descent parser encodes state information in its run-time stack, or call stack
-> using recursive procedure calls to implement a stack abstraction may not be particularly efficient
-> this suggests other implementation methods
	- explicit stack, hand-coded parser
	- stack-based, table-driven parser

- now a predictive parser looks like
	source code -> 	scanner --tokens--> 
					stack ------------>	IR
					parsing tables <-->

- rather than writing code, we build TABLES
		[building tables can be automated!]

Table-driven parsers
- a parser generator sytem often looks like:
	source code -> 	scanner ------tokens----------> 
								stack ------------>	IR
	grammar->parser generator-> parsing tables <-->

- this is true for both top-down (LL) and bottom-up (LR) parsers

Non-recursive predictive parsing
- input: a string w and a parsing table M for G
	
	tos <- 0
	Stack[tos] <- EOF
	Stack[++tos] <- StartSymbol
	token <- next_token()
	repeat
			X <- Stack[tos]
			if X is a terminal or EOF
				if X = token then
					pop X
					token <- next_token()
				else error
			else // X is a non-terminal
				if M[X, token] = X -> Y1 Y2 ... Yk
					pop X
					push Yk, Yk-1, ... , Y1
				else error()
	until X = EOF

- What we need now is a parsing table M.
	-> ~~pag48 -> how can i generate it?

FIRST
- for a string of grammar symbols a (a€V*), define FIRST(a) as:
	-> the set of terminal symbols that begin strings derived from a: {w€Vt, a,b€V* | a=>*wb}
	-> if a=>* epsilon then epsilon € FIRST(a)
- FIRST(a) contains the set of tokens valid in the initial position in a. To build FIRST(X):
	(1)	if X€Vt then FIRST(X) is {X}
	(2) if X->epsilon then add epsilon in FIRST(X)
	(3) if X-> Y1 Y2 ... Yk
		(a) put FIRST(Y1)-{epsilon} in FIRST(X)
		(b)	for all i: 1<i<=k, if epsilon € to 
		FIRST(Y1) intersecated ... intersecated FIRST(Yi-1)
		then put FIRST(Yi)-{epsilon} in FIRST(X)
		(c) if epsilon € FIRST(Y1) intersecated ... intersecated FIRST(Yk) then put epsilon in FIRST(X)
	repeat until no more additions can be made.

FOLLOW
- for a non-terminal A, define FOLLOW(A) as:
	-> the set of terminals that can appear immediately to the right of A in some sentential form
	-> i.e., a non-terminal's FOLLOW set specifies the tokens that can legally appear after it
	-> a terminal symbol has no FOLLOW set (since don't exist sentential form derived from it)
- To build FOLLOW(A):
	(1)	put $ (EOF) in FOLLOW(<goal>)
	(2)	if A -> aBb: (where a€V*, B€Vn, b€V*)
		(a) put FIRST(b)-{epsilon} in FOLLOW(B)
		(b) if b = epsilon (i.e., A->aB) or epsilon€FIRST(b) (i.e., b=>*epsilon) then put FOLLOW(A) in FOLLOW(B)
	repeat until no more additions can be made
 
LL(1) grammars
- Previous definition:
	-> a grammar G is LL(1) iff for all non-terminals A, each distinct pair of productions A -> b and A -> y (b,y€V*) satisfy the condition FIRST(b) intersecated FIRST(y) = 0
-> but what if A=>*epsilon ???

- Revised definition:
	-> a grammar G is LL(1) iff for each set of productions 
	A->a1 | a2 | ... | an where (a1,a2,...,an€V*)
		(1) FIRST(a1), FIRST(a2),...,FIRST(an) are pairwise disjoint
		(2) if ai =>* epsilon then 
			FIRST(aj) intersecated FOLLOW(A) = 0 
			for all 1<=j<=n and i!=j
	NB: if G is epsilon-free, condition 1 is sufficient.

	[FOLLOW(A) must be disjoint from FIRST(aj), else we do not know wheter to go to aj or take ai and skip to what follows]

Properties of LL(1) grammars
	1. no left-recursive grammar is LL(1)
	2. no ambiguous grammar is LL(1)
	3. some languages have no LL(1) grammar
	4. a epsilon-free grammar where each alternative expansion for A begins with a distinct terminal is a simple LL(1) grammar

LL(1) parse table construction
- input: grammar G,
- output: parsing table M,
- Method:
	1. for all production A->a (a€V*)
		(a.) for all x€FIRST(a) (x€Vt), add A->a to M[A,x]
		(b.) if epsilon € FIRST(a):
			i. for all y€FOLLOW(A) (y€Vt), add A->a to M[A,y]
			ii. if $€FOLLOW(A), add A->a to M[A,$]
	2. set each undefined entry of M to 'ERROR'

- if exists M[A,x] with multiple entries then G is not LL(1)
		-> x,y€Vt , thus x,y != epsilon
Example
-> pag 54

A grammar that is not LL(1)
	<stmt> ::=	if <expr> then <stmt>
			|	if <expr> then <stmt> else <stmt>
			|	...

	left factored:
	<stmt> ::=	if <expr> then <stmt><stmt'>
			|	else <stmt> | epsilon
			|	...

	now FIRST(<stmt'>) = {epsilon, else}
	also, FOLLOW(<stmt'>)= {else, $}
	thus FIRST(<stmt'>)intersecatedFIRST(<stmt'>)={else} != 0

	on seeing 'else', conflict between choosing
		<stmt'>::=else<stmt> and <stmt'>::=epsilon

Error recovery
- key notion
	-> for each non-terminal, construct a set of terminals on which the parser can synchronize
	-> when an error occurs looking for A, scan until an element of SYNC(A) is found
- building SYNC(A)
	1. a€FOLLOW(A) => a€SYNC(A) (a€Vt)
	2. place keywords that start statemets in SYNC(A)
	3. add symbols in FIRST(A) to SYNC(A)
- if we can't match a terminal on top of stack:
	1. pop the terminal
	2. print a message saying the terminal was inserted
	3. continue the parse
	i.e. SYNC(a) = Vt - {a}